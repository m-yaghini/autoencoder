{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia as wp\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from operator import concat\n",
    "from functools import reduce #python 3\n",
    "from sklearn import svm\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pilot data (10 classes 100 articles per class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Re-formatting and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./data.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['X'].tolist()\n",
    "y = data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_documents_matrix(docs_mat):\n",
    "    # paritally adopted from gensim documentation\n",
    "    \n",
    "    from string import punctuation\n",
    "    punctuation = set(punctuation)\n",
    "\n",
    "    docs_list = docs_mat.tolist()\n",
    "    \n",
    "    # remove common words and tokenize\n",
    "    stoplist = set('for a of the and to in'.split())\n",
    "\n",
    "    texts = [[''.join(ch for ch in word if ch not in punctuation)\n",
    "              for word in (document.lower().split())\n",
    "               if word not in stoplist]\n",
    "              for document in docs_list]\n",
    "\n",
    "    # remove words that appear only once\n",
    "    from collections import defaultdict\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "    texts = [[token for token in text if frequency[token] > 1]\n",
    "             for text in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cleaned = preprocess_documents_matrix(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Counter()\n",
    "words.update(reduce(concat, X_train_cleaned))\n",
    "freq = words.most_common(2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [item[0] for item in freq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Docs-Vocabulary Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_vocab_freq_matrix(docs, vocab):\n",
    "    from collections import Counter\n",
    "\n",
    "    docs_freq_distribution = []\n",
    "    for doc in docs:\n",
    "        word_freq = Counter()\n",
    "        word_freq.update(doc)\n",
    "        doc_freq_dist = [word_freq[word] for word in vocab]\n",
    "        docs_freq_distribution.append(doc_freq_dist)\n",
    "    return docs_freq_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_freq_dist = docs_vocab_freq_matrix(X_train_cleaned, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_freq_dist = np.array(docs_freq_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(docs_freq_dist, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cleaned = preprocess_documents_matrix(X_test)\n",
    "test_docs_freq_dist = docs_vocab_freq_matrix(X_test_cleaned, vocab)\n",
    "test_docs_freq_dist = np.array(test_docs_freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76000000000000001"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_docs_freq_dist, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving test/train splits for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('./pilot_data_test_train_split', \n",
    "                   X_train = docs_freq_dist, \n",
    "                   X_test = test_docs_freq_dist,\n",
    "                   y_train = y_train,\n",
    "                   y_test = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./pilot_data_test_train_split.npz')\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def compress_reconstruct_with_PCA(X, n_components=150):\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X)\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    reconstructed_X = np.dot(pca.transform(X)[:,:n_components], \n",
    "                             pca.components_[:n_components,:]) + X\n",
    "    return reconstructed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_X_train = compress_reconstruct_with_PCA(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PCA Reconstructed Score: 0.6966666666666667\n"
     ]
    }
   ],
   "source": [
    "clf_reconstructed = LogisticRegression()\n",
    "clf_reconstructed.fit(reconstructed_X_train, y_train)\n",
    "\n",
    "reconstructed_X_test = compress_reconstruct_with_PCA(X_test)\n",
    "PCA_reconstruction_score = clf_reconstructed.score(reconstructed_X_test, y_test)\n",
    "print(\"=== PCA Reconstructed Score: {}\".format(PCA_reconstruction_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing to make the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading raw scrapped data\n",
    "data = np.load('./data_10class_1000perClass_2.npz')\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Preprocessing and cleaning\n",
    "X_train_cleaned = preprocess_documents_matrix(X_train)\n",
    "\n",
    "# Building the vocabulary\n",
    "words = Counter()\n",
    "words.update(reduce(concat, X_train_cleaned))\n",
    "freq = words.most_common(2000)\n",
    "vocab = [item[0] for item in freq]\n",
    "\n",
    "# Creating the Docs-Vocabulary Matrix\n",
    "docs_freq_dist = docs_vocab_freq_matrix(X_train_cleaned, vocab)\n",
    "docs_freq_dist = np.array(docs_freq_dist)\n",
    "X_test_cleaned = preprocess_documents_matrix(X_test)\n",
    "test_docs_freq_dist = docs_vocab_freq_matrix(X_test_cleaned, vocab)\n",
    "test_docs_freq_dist = np.array(test_docs_freq_dist)\n",
    "\n",
    "\n",
    "# Saving splits\n",
    "np.savez_compressed('./full_data_test_train_split', \n",
    "                   X_train = docs_freq_dist, \n",
    "                   X_test = test_docs_freq_dist,\n",
    "                   y_train = y_train,\n",
    "                   y_test = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./full_data_test_train_split.npz')\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_X_train = compress_reconstruct_with_PCA(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PCA Reconstructed Score: 0.6773333333333333\n"
     ]
    }
   ],
   "source": [
    "clf_reconstructed = LogisticRegression()\n",
    "clf_reconstructed.fit(reconstructed_X_train, y_train)\n",
    "\n",
    "reconstructed_X_test = compress_reconstruct_with_PCA(X_test)\n",
    "PCA_reconstruction_score = clf_reconstructed.score(reconstructed_X_test, y_test)\n",
    "print(\"=== PCA Reconstructed Score: {}\".format(PCA_reconstruction_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PCA Reconstructed Score (SVM): 0.624\n"
     ]
    }
   ],
   "source": [
    "clf_reconstructed_svm = svm.LinearSVC()\n",
    "clf_reconstructed_svm.fit(reconstructed_X_train, y_train)\n",
    "\n",
    "reconstructed_X_test = compress_reconstruct_with_PCA(X_test)\n",
    "PCA_reconstruction_score = clf_reconstructed_svm.score(reconstructed_X_test, y_test)\n",
    "print(\"=== PCA Reconstructed Score (SVM): {}\".format(PCA_reconstruction_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with most common word distributions did not turn out so well. Even on the full dataset, I got around 16% accuracy which is barely higher than the random classifier (10%) on this task.\n",
    "\n",
    "Upon inspections of the model, researching the community and reviewing the literature I found two main reasons for this:\n",
    "\n",
    "1. Weight initializations\n",
    "2. Text-data pre-processing\n",
    "\n",
    "My initial input pre-processing was based on the experiments section of [this article](https://arxiv.org/abs/1705.02033), which uses the 2000 most frequent words as vocabulary (of course after removal of stop words and punctuation). However, given the poor performance of the end-to-end system using those features, I decided to use word-embeddings which boosted the classifier performance to 54%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight initialization has always been an issue since the inception of autoencoders. So much so that in a famous paper in [Science](https://www.cs.toronto.edu/~hinton/science.pdf), Hinton et al. discuss using a Restricted Boltzmann machine precisely to find good initialization for the weights.\n",
    "\n",
    "Given the recent progress in deep neural networks training, specifically that of [Batch Norm](https://arxiv.org/abs/1502.03167), I decided to add batch norm layers after each linear layer, because they are known to reduce the effect of bad initialization in the training of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model = KeyedVectors.load_word2vec_format('./wiki.en/wiki.en.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_embeddings_mat(X, y):\n",
    "    docs_embeddings_list = []\n",
    "    docs_labels_list = []\n",
    "    for doc_ind, doc in enumerate(X):\n",
    "        word_embeddings_list = []\n",
    "        for word in doc:\n",
    "            try:\n",
    "                word_embedding = en_model.get_vector(word)\n",
    "                word_embeddings_list.append(word_embedding)\n",
    "            except KeyError:\n",
    "                continue\n",
    "        try: \n",
    "            doc_embedding = np.mean(word_embeddings_list, axis=0)\n",
    "            docs_embeddings_list.append(doc_embedding)\n",
    "            docs_labels_list.append(y[doc_ind])\n",
    "        except FloatingPointError:\n",
    "            continue\n",
    "    docs_embeddings_mat = np.vstack(docs_embeddings_list)\n",
    "    docs_labels = np.hstack(docs_labels_list)\n",
    "    return docs_embeddings_mat, docs_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammad/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "np.seterr(all='raise')\n",
    "\n",
    "# Loading raw scrapped data\n",
    "data = np.load('./data_10class_1000perClass_2.npz')\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Preprocessing and cleaning\n",
    "X_train_cleaned = preprocess_documents_matrix(X_train)\n",
    "X_test_cleaned = preprocess_documents_matrix(X_test)\n",
    "\n",
    "# Creating the Docs-Embeddings Matrix\n",
    "train_docs_embeddings_mat, train_labels = docs_embeddings_mat(X_train_cleaned, y_train)\n",
    "test_docs_embeddings_mat, test_labels = docs_embeddings_mat(X_test_cleaned, y_test)\n",
    "\n",
    "# Saving splits\n",
    "np.savez_compressed('./full_data_test_train_split_with_embeddings', \n",
    "                   X_train = train_docs_embeddings_mat, \n",
    "                   X_test = test_docs_embeddings_mat,\n",
    "                   y_train = train_labels,\n",
    "                   y_test = test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6988,)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with reconstructed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./reconstructed_train.npz')\n",
    "train_reconstructed_features = data['train_reconstructed_features']\n",
    "train_labels = data['train_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_reconstructed = LogisticRegression(verbose=True, solver='sag', n_jobs=4, tol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 5 epochs took 378 seconds\n",
      "convergence after 6 epochs took 452 seconds\n",
      "convergence after 6 epochs took 503 seconds\n",
      "convergence after 7 epochs took 524 seconds\n",
      "convergence after 6 epochs took 463 seconds\n",
      "convergence after 5 epochs took 380 seconds\n",
      "convergence after 5 epochs took 413 seconds\n",
      "convergence after 7 epochs took 483 seconds\n",
      "convergence after 5 epochs took 114 seconds\n",
      "convergence after 5 epochs took 56 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed: 16.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=4,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.1,\n",
       "          verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_reconstructed.fit(train_reconstructed_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = np.load('./reconstructed_test.npz')\n",
    "reconstructed_test_features = data_test['test_reconstructed_features']\n",
    "test_labels = data_test['test_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Reconstructed Score: 0.6645548516172057\n"
     ]
    }
   ],
   "source": [
    "reconstructed_score = clf_reconstructed.score(reconstructed_test_features, test_labels)\n",
    "print(\"=== Reconstructed Score: {}\".format(reconstructed_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline (with embeddings): Original Signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the splits\n",
    "data = np.load('./full_data_test_train_split_with_embeddings.npz')\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_original = LogisticRegression(verbose=True, solver='sag', n_jobs=4, tol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 4 epochs took 1 seconds\n",
      "convergence after 4 epochs took 1 seconds\n",
      "convergence after 4 epochs took 1 seconds\n",
      "convergence after 5 epochs took 2 seconds\n",
      "convergence after 5 epochs took 2 seconds\n",
      "convergence after 5 epochs took 2 seconds\n",
      "convergence after 5 epochs took 2 seconds\n",
      "convergence after 4 epochs took 1 seconds\n",
      "convergence after 4 epochs took 0 seconds\n",
      "convergence after 5 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=4,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.1,\n",
       "          verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_original.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Original Score (with embeddingS): 0.7342447482494164\n"
     ]
    }
   ],
   "source": [
    "original_score = clf_original.score(X_test, y_test)\n",
    "print(\"=== Original Score (with embeddingS): {}\".format(original_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline (with embeddings): 50-PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_reconstructed = compress_reconstruct_with_PCA(X_train, n_components=50)\n",
    "X_test_pca_reconstructed = compress_reconstruct_with_PCA(X_test, n_components=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 4 epochs took 1 seconds\n",
      "convergence after 4 epochs took 1 seconds\n",
      "convergence after 5 epochs took 1 seconds\n",
      "convergence after 5 epochs took 1 seconds\n",
      "convergence after 4 epochs took 1 seconds\n",
      "convergence after 5 epochs took 2 seconds\n",
      "convergence after 4 epochs took 2 seconds\n",
      "convergence after 5 epochs took 2 seconds\n",
      "convergence after 5 epochs took 1 seconds\n",
      "convergence after 5 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=4,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.1,\n",
       "          verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pca_reconstructed = LogisticRegression(verbose=True, solver='sag', n_jobs=4, tol=0.1)\n",
    "clf_pca_reconstructed.fit(X_train_pca_reconstructed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PCA Reconstructed Score (with embeddings): 0.7262420806935646\n"
     ]
    }
   ],
   "source": [
    "pca_reconstruction_score = clf_pca_reconstructed.score(X_test_pca_reconstructed, y_test)\n",
    "print(\"=== PCA Reconstructed Score (with embeddings): {}\".format(pca_reconstruction_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
